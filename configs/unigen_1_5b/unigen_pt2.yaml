wandb:
  entity: null
  resume: 'auto'

experiment:
    project: 'tuning'
    train_stage: 'stage2'
    max_train_examples_t2i: 35000000
    max_train_examples_mmu: 36000000
    save_every: 20000
    eval_every: 2500
    generate_every: 10000
    log_every: 50
    log_grad_norm_every: 500
    resume_from_checkpoint: 'latest'
    eval_batch_size: 16

model:
    vq_model:
        type: 'magvitv2'
        vq_model_name: 'magvitv2'

    unigen:
        load_from_pretrained: True
        llm_model_path: 'Qwen2.5-1.5B-Instruct'
        w_und_encoder: False
        codebook_size: 8192
        num_vq_tokens: 256
    
    gradient_checkpointing: True
    task_token_first: False

dataset:
    gen_type: 't2i'
    und_type: 'captioning'
    combined_loader_mode: 'max_size_cycle'
    params:
        train_t2i_shards_path_or_url: [
          'imagenet21k_recap2_5_long/train-{00000..07769}.tar',
          'sa1b_recap2_5_long_512/data_000{00000..63999}.tar',
          'cc3m_recap2_5_long/cc3m-train-{0000..0575}.tar',
          'cc12m_recap2_5_long/cc12m-train-{0000..2175}.tar'
        ]
        train_lm_shards_path_or_url: [
          'falcon-refinedweb/data'
        ]
        train_mmu_shards_path_or_url:  [
          'imagenet21k_recap2_5_long/train-{00000..07769}.tar',
          'sa1b_recap2_5_long_512/data_000{00000..63999}.tar',
          'cc3m_recap2_5_long/cc3m-train-{0000..0575}.tar',
          'cc12m_recap2_5_long/cc12m-train-{0000..2175}.tar'
        ]
        train_lm_subsample: 1.0
        add_caption_prompt: True
        validation_prompts_file: 'data/prompts/unigen_prompts_val.txt'
        t2i_short_caption_ratio: 0.5
        shuffle_buffer_size: 1000
        num_workers: 16
        resolution: 256
        pin_memory: True
        persistent_workers: True
        caption_file: 'data/prompts/long_caption_prompt.json'

    preprocessing:
        max_seq_length: 576 # for text tokens
        resolution: 256
        center_crop: False
        random_flip: False

optimizer:
    name: adamw
    params: # default adamw params
        learning_rate: 1e-4
        scale_lr: False # scale learning rate by total batch size
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0.01
        epsilon: 1e-8

lr_scheduler:
    scheduler: 'cosine'
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_steps: 5000
        min_lr: 1e-7

training:
    gradient_accumulation_steps: 1
    noise_type: 'mask'
    batch_size_t2i: 8
    batch_size_lm: 2
    batch_size_mmu: 6
    mixed_precision: 'bf16'
    enable_tf32: True
    seed: 10086
    max_train_steps: 400000
    overfit_one_batch: False
    cond_dropout_prob: 0.1
    min_masking_rate: 0.0
    label_smoothing: 0.0
    max_grad_norm: null
    guidance_scale: 2.0
    generation_timesteps: 16
    t2i_coeff: 1.0
    lm_coeff: 0.1
    mmu_coeff: 1.0
