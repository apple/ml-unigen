#
# For licensing see accompanying LICENSE file.
# Copyright (C) 2025 Apple Inc. All Rights Reserved.
#

import os
import re
import sys

import math
import json
import glob
import shutil
from pathlib import Path

os.environ["TOKENIZERS_PARALLELISM"] = "true"

current_file_path = Path(__file__).resolve()
sys.path.insert(0, str(current_file_path.parent.parent.parent))

from PIL import Image
from tqdm import tqdm, trange
from einops import rearrange
import numpy as np
import pandas as pd
import torch
import torch.distributed as dist
from torchvision.utils import make_grid
from accelerate import PartialState
from transformers import AutoTokenizer
from pytorch_lightning import seed_everything

from data.llava.conversation import conv_templates
from training.prompting_utils import (
    UniversalPromptingQwen2,
    create_attention_mask_predict_next, create_attention_mask_for_mmu, create_attention_mask_for_mmu_vit
)
from data.transform import image_transform
from models import get_mask_chedule
from models.model_registry import model_from_name, get_model_creator
from utils.configuration import initialize_config


QWEN_TEMPLATE = (
    "<|im_start|>user\n"
    "{user_prompt}<|im_end|>\n"
    "<|im_start|>assistant\n"
)
QWEN_SYSTEM_PROMPT = "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
MMU_RATING_USER_PROMPT = {
    "outcome": "This image is generated by a prompt: {context}. Does this image accurately represent the prompt? Please answer yes or no.",
    "think": "This image is generated by a prompt: {context}. Please assess the image generation quality step by step. First, breakdown the prompt into multiple visual questions and iteratively answer each question with Yes or No between <think_start> <think_end>. Questions should cover all-round details about whether the image accurately represents entity categories, counting of entities, color, spatial relationship in the prompt. Next, output the final result between <answer_start> <answer_end>. Output Yes if all multi-choice answers equal yes to show the image has accurate alignment with the prompt. Otherwise answer with No.",
    "rule": "{context}  Please answer yes or no."
}

def load_vision_encoders(config, device):
    vq_model = get_model_creator(config.model.vq_model.type).from_pretrained(
        config.model.vq_model.vq_model_name
    ).to(device)
    vq_model.requires_grad_(False)
    vq_model.eval()

    vision_tower = model_from_name(config.model.vision_tower.name).to(device)
    vision_tower.requires_grad_(False)
    vision_tower.eval()
    return vq_model, vision_tower


def load_uni_prompting(tokenizer, model_version, max_len, config):
    uni_prompting = None
    max_len_mode = config.model.get("max_len_mode", 'text')
    uni_prompting = UniversalPromptingQwen2(
        tokenizer,
        special_tokens=(
            "<|soi|>", "<|eoi|>", "<|sov|>", "<|eov|>", "<|t2i|>",
            "<|mmu|>", "<|t2v|>", "<|v2v|>", "<|lvg|>"
        ),
        max_seq_len=(
            config.dataset.preprocessing.max_seq_length + config.model.unigen.num_vq_tokens + 3
            if max_len_mode == 'text'
            else max_len
        ),
        # computing the maximum length of full sequence
        enable_reuse_tk=config.model.get("enable_reuse_tk", False),
        # reuse similar tokens in Qwen2 template, e.g., <|vision_start|>, <|vision_end|>
        task_token_first=config.model.get("task_token_first", True),
        ignore_id=-100, cond_dropout_prob=config.training.cond_dropout_prob
    )
    return uni_prompting


def load_unigen(config, device):
    use_safetensors = config.model.get('load_with_safetensors', None)
    ckpt_base_path = config.model.get("local_checkpoints", "")
    pretrained_model_path =os.path.join(ckpt_base_path, config.model.unigen.pretrained_model_path)
    
    print(f"load with local model: {pretrained_model_path}, use_safetensors: {use_safetensors}")
    if not pretrained_model_path.endswith("unwrapped_model"):
        if os.path.exists(os.path.join(pretrained_model_path, 'unwrapped_model')):
            pretrained_model_path = os.path.join(pretrained_model_path, 'unwrapped_model')
        else:
            ckpt_files = glob.glob(os.path.join(pretrained_model_path, "*/unwrapped_model"))
            ckpt_files.sort(key=os.path.getmtime, reverse=True)
            pretrained_model_path = ckpt_files[0]
    if len(glob.glob(os.path.join(pretrained_model_path, "*.safetensors"))) > 0:
        use_safetensors = True
    elif len(glob.glob(os.path.join(pretrained_model_path, "pytorch_model*.bin"))) > 0:
        use_safetensors = False

    # map the substrings to the version labels
    # (version, max_len, padding_side)
    version_map = {
        "Qwen2.5": ("qwen_2.5", 32_768, "right"),
    }
    # pick the first matching version, or None if no key is found
    model_version, max_len, padding_side = next(
        (version for substr, version in version_map.items() if substr in config.model.unigen.llm_model_path),
        None
    )
    max_len = config.model.unigen.get("model_max_length", max_len)
    tokenizer = AutoTokenizer.from_pretrained(
        config.model.unigen.llm_model_path,
        model_max_length=max_len,
        padding_side=padding_side
    )

    uni_prompting = load_uni_prompting(tokenizer, model_version, max_len, config)
    if "qwen" in model_version:
        config.model.unigen.vocab_size = len(uni_prompting.text_tokenizer) + config.model.unigen.codebook_size + 1
        config.model.unigen.llm_vocab_size = uni_prompting.text_tokenizer.vocab_size
        config.model.unigen.num_new_special_tokens = (
            len(uni_prompting.text_tokenizer) - config.model.unigen.llm_vocab_size
        )

    ckpt_base_path = config.model.get("local_checkpoints", "")
    model = get_model_creator('unigen').from_pretrained(
        pretrained_model_path,
        use_safetensors=use_safetensors,
        ckpt_base_path=ckpt_base_path,
    ).to(device)
    model.eval()

    return model, uni_prompting, model_version


def generate_images(uni_model, uni_prompting, vq_model, config, device, prompts, distributed_state, output_dir):
    num_proc = distributed_state.num_processes
    t2i_gen_mode = config.model.get('t2i_gen_mode', 'mask')
    n_samples = config.inference.get("n_samples", 4)
    eval_text_len =  config.inference.get("eval_text_len", 128)
    batch_size = min(n_samples, 10)
    rounds = math.ceil(n_samples / batch_size)
    use_causal_mask = config.model.get('use_causal_mask', False)
    mask_token_id = uni_model.config.mask_token_id
    assert use_causal_mask == True or t2i_gen_mode == 'mask'

    for round_id in range(rounds):
        for batch_index in trange(0, len(prompts), num_proc, desc=f"Generating images"):
            batch_index_prompt = batch_index % len(prompts)
            batch_raw = prompts[batch_index_prompt: batch_index_prompt + num_proc]
            with distributed_state.split_between_processes(batch_raw) as batch:
                prompt_idx = batch_index_prompt + distributed_state.process_index
                if batch is not None and prompt_idx < len(prompts):
                    prompt_idx_name = batch[0]['item_id'] if 'item_id' in batch[0] else f"{prompt_idx:0>5}"
                    outpath = os.path.join(output_dir, prompt_idx_name)
                    os.makedirs(outpath, exist_ok=True)
                    prompt = [batch[0]['prompt']] * batch_size if 'prompt' in batch[0] else [batch[0]['text']] * batch_size 
                    print(f"[round-{round_id + 1}/{rounds}] Prompt ({prompt_idx: >3}/{len(prompts)}): '{prompt[0]}'")
                    sample_path = os.path.join(outpath, "samples")
                    os.makedirs(sample_path, exist_ok=True)
                    with open(os.path.join(outpath, "metadata.jsonl"), "w") as fp:
                        json.dump(batch[0], fp)
                    sample_count = round_id * batch_size
                    image_tokens = torch.ones(
                        (len(prompt), config.model.unigen.num_vq_tokens), dtype=torch.long,
                        device=device) * mask_token_id
                    input_ids, attention_mask = uni_prompting((prompt, image_tokens, eval_text_len), 't2i_gen')

                    if config.training.guidance_scale > 0:
                        uncond_input_ids, attention_mask_ = uni_prompting(([''] * len(prompt), image_tokens, eval_text_len), 't2i_gen')
                        if use_causal_mask:
                            attention_mask = torch.cat([attention_mask, attention_mask_], dim=0)
                        else:
                            attention_mask = create_attention_mask_predict_next(
                                torch.cat([input_ids, uncond_input_ids], dim=0),
                                pad_id=int(uni_prompting.sptids_dict['<|pad|>']),
                                soi_id=int(uni_prompting.sptids_dict['<|soi|>']),
                                eoi_id=int(uni_prompting.sptids_dict['<|eoi|>']),
                                rm_pad_in_image=True)
                    else:
                        if use_causal_mask:
                            attention_mask = attention_mask
                        else:
                            attention_mask = create_attention_mask_predict_next(
                                input_ids,
                                pad_id=int(uni_prompting.sptids_dict['<|pad|>']),
                                soi_id=int(uni_prompting.sptids_dict['<|soi|>']),
                                eoi_id=int(uni_prompting.sptids_dict['<|eoi|>']),
                                rm_pad_in_image=True
                            )
                        uncond_input_ids = None

                    if config.get("mask_schedule", None) is not None:
                        schedule = config.mask_schedule.schedule
                        args = config.mask_schedule.get("params", {})
                        mask_schedule = get_mask_chedule(schedule, **args)
                    else:
                        mask_schedule = get_mask_chedule(config.training.get("mask_schedule", "cosine"))

                    with torch.no_grad():
                        if t2i_gen_mode == 'mask':
                            gen_token_ids = uni_model.t2i_generate(
                                input_ids=input_ids,
                                uncond_input_ids=uncond_input_ids,
                                attention_mask=attention_mask,
                                guidance_scale=config.training.guidance_scale,
                                temperature=config.training.get("generation_temperature", 1.0),
                                timesteps=config.training.generation_timesteps,
                                noise_schedule=mask_schedule,
                                text_vocab_size=len(uni_prompting.text_tokenizer),
                                image_token_num_per_image=config.model.unigen.num_vq_tokens,
                            )
                        elif t2i_gen_mode == 'ar':
                            gen_token_ids = uni_model.t2i_generate_ar(
                                input_ids=input_ids,
                                uncond_input_ids=uncond_input_ids,
                                attention_mask=attention_mask,
                                guidance_scale=config.training.guidance_scale,
                                temperature=config.training.get("generation_temperature", 1.0),
                                text_vocab_size=len(uni_prompting.text_tokenizer),
                                image_token_num_per_image=config.model.unigen.num_vq_tokens,
                            )

                    gen_token_ids = torch.clamp(gen_token_ids, max=config.model.unigen.codebook_size - 1, min=0)
                    images = vq_model.decode_code(gen_token_ids)
                    images = torch.clamp((images + 1.0) / 2.0, min=0.0, max=1.0)
                    
                    images *= 255.0
                    images = images.permute(0, 2, 3, 1).cpu().numpy().astype(np.uint8) 
                    for image in images:
                        image = Image.fromarray(image)
                        image.save(os.path.join(sample_path, f"{sample_count:05}.png"))
                        sample_count += 1


def parse_yesno(answer):
    YESNO_RE = re.compile(r'^\s*(yes|no)\b', re.IGNORECASE)
    m = YESNO_RE.match(answer)
    if not m:
        return 'no'
    return m.group(1).lower()


def parse_qa_pairs(text, start_token='<think_start>', end_token='<think_end>'):
    if start_token not in text:
        text = start_token + text
    if end_token not in text:
        text = text + end_token
    pattern = rf"{re.escape(start_token)}(.*?){re.escape(end_token)}"
    think_section = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
    if not think_section:
        return [('', 'no')] # No think section found
    think_text = think_section.group(1)
    pattern = re.compile(r"(.*?\?)(?:\s*)(yes|no)\b", flags=re.IGNORECASE | re.DOTALL)
    matches = pattern.findall(think_text)
    qa_pairs = []
    if len(matches) == 0:
        return [('', 'no')] # No think section found
    for question, answer in matches:
        question = question.strip()
        question = re.sub(r"^[;.\s]+", "", question)
        if not question.endswith('?'):
            question += '?'
        qa_pairs.append((question, answer.lower()))
    return qa_pairs


def generate_mmu_rating(uni_model, uni_prompting, vq_model, vision_tower, config, device,
                        mmu_prompt_style, generated_image_dir):
    if dist.is_available() and dist.is_initialized():
        rank = dist.get_rank()
        world_size = dist.get_world_size()
    else:
        rank = int(os.getenv("LOCAL_RANK", 0))
        world_size = int(os.getenv("WORLD_SIZE", 1))

    all_images = sorted(Path(generated_image_dir).glob("**/*.png"))
    total = len(all_images)
    image_list = all_images[rank::world_size]

    print(f"[RANK {rank + 1}/{world_size}]: processing {len(image_list)}/{total} images...")

    # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions
    temperature = 0.8
    # retain only the top_k most likely tokens, clamp others to have 0 probability
    top_k = 1
    if config.inference.get('benchmark', 'geneval') =='dpg' and mmu_prompt_style  in ["breakdown", "rule"]:
        item_questions_dict = {}
        with open(config.dataset.question_file, encoding="utf-8") as fp:
            for line in fp:
                entry = json.loads(line)
                item_id = entry["item_id"]
                questions = entry["questions"]
                item_questions_dict[item_id] = questions    
                    
    image_processor = vision_tower.image_processor
    responses = []
    for i, image_file in enumerate(tqdm(image_list)):
        with open(image_file.parent.parent / "metadata.jsonl", "r", encoding="utf-8") as fp:
            metadata = [json.loads(l.strip()) for l in fp]
        prompt = metadata[0]["prompt"] if 'prompt' in metadata[0] else metadata[0]["text"]
        if config.inference.get('benchmark', 'geneval') == 'dpg':
            image_id =  image_file.relative_to(generated_image_dir).as_posix().split('/')[0]
            questions = item_questions_dict[image_id] if mmu_prompt_style in ["breakdown", "rule"] else [prompt]
        else:
            questions = geneval_prompt2questions(prompt) if mmu_prompt_style in ["breakdown", "rule"] else [prompt]
        image_ori = Image.open(image_file).convert("RGB")
        image = image_transform(image_ori, resolution=config.dataset.params.resolution).to(device)
        image = image.unsqueeze(0)
        batch_size = 1

        user_prompt_template = MMU_RATING_USER_PROMPT[mmu_prompt_style]
        questions = [user_prompt_template.format(context=q) for q in questions] # .replace('a photo of', '')
        questions = [QWEN_TEMPLATE.format(user_prompt=q) for q in questions]
        for prompt_question in questions:
            question_input = [prompt_question.strip()]
            input_ids = [
                uni_prompting.text_tokenizer(prompt, return_tensors="pt", padding="longest").input_ids
                for prompt in question_input
            ]
            input_ids = torch.stack(input_ids)

            if config.model.unigen.w_und_encoder:
                image_tensor = image_processor(image_ori, return_tensors="pt")["pixel_values"][0].unsqueeze(0)
                images_feat = vision_tower(image_tensor.to(device))
                images_embeddings = uni_model.mm_projector(images_feat)  # B*n , N, C

                input_ids = torch.nn.utils.rnn.pad_sequence(
                    input_ids, batch_first=True, padding_value=uni_prompting.text_tokenizer.pad_token_id
                )
                input_ids = torch.tensor(input_ids).to(device)
                input_ids_system = [
                    uni_prompting.text_tokenizer(
                        QWEN_SYSTEM_PROMPT, return_tensors="pt", padding="longest").input_ids
                    for _ in range(batch_size)
                ]
                input_ids_system = torch.stack(input_ids_system, dim=0)
                input_ids_system = input_ids_system.to(device)
                input_ids_system = input_ids_system[0]

                input_ids = torch.tensor(input_ids).to(device).squeeze(0)
                input_ids_part1, input_ids_part2, attention_mask, _ = uni_prompting((images_feat, input_ids, None, input_ids_system), 'mmu_conv')
                part1 = uni_model.unigen.model.embed_tokens(input_ids_part1)
                part2 = uni_model.unigen.model.embed_tokens(input_ids_part2)
                # Full input seq
                input_embeddings = torch.cat((part1, images_embeddings, part2), dim=1)

                if config.model.get('use_causal_mask', False) or config.model.get('use_causal_mask_mmu', False):
                    attention_mask_llava = attention_mask.to(torch.bool)
                    cont_toks_list = uni_model.generate(
                        input_embeddings=input_embeddings,
                        # attention_mask=attention_mask_llava,
                        do_sample=True if temperature < 1.0 else False,
                        temperature=temperature,
                        # top_p=gen_kwargs["top_p"],
                        # num_beams=gen_kwargs["num_beams"],
                        max_new_tokens=config.model.max_new_tokens,
                        use_cache=True,
                        pad_token_id=uni_prompting.text_tokenizer.eos_token_id,
                    )
                else:
                    attention_mask_llava = create_attention_mask_for_mmu_vit(input_embeddings,
                                                                             prefix_length=input_ids_part1.shape[1],
                                                                             num_tokens=config.model.vision_tower.num_tokens,
                                                                             num_images=1)
                    cont_toks_list = uni_model.mmu_generate(
                        input_embeddings=input_embeddings,
                        attention_mask=attention_mask_llava[0].unsqueeze(0),
                        max_new_tokens=config.model.max_new_tokens,
                        top_k=top_k,
                        eot_token=uni_prompting.text_tokenizer.eos_token_id
                    )
                    cont_toks_list = torch.stack(cont_toks_list).squeeze()[None]
            else:
                image_tokens = vq_model.get_code(image) + len(uni_prompting.text_tokenizer)
                input_ids, attention_mask, _ = uni_prompting((image_tokens, input_ids, None, None), 'mmu_conv')
                attention_mask = create_attention_mask_for_mmu(input_ids.to(device),
                                                               eoi_id=int(uni_prompting.sptids_dict['<|eoi|>']))
                cont_toks_list = uni_model.mmu_generate(
                    input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=config.model.max_new_tokens,
                    top_k=top_k,
                    eot_token=uni_prompting.text_tokenizer.eos_token_id
                )
                cont_toks_list = torch.stack(cont_toks_list).squeeze()[None]

            text_outputs = uni_prompting.text_tokenizer.batch_decode(cont_toks_list, skip_special_tokens=True)
            if mmu_prompt_style != 'think':
                responses.append({
                    'image_id': image_file.relative_to(generated_image_dir).as_posix(),
                    'question': prompt_question, 'raw_answer': text_outputs[0].strip(),
                    'answer': parse_yesno(text_outputs[0].strip()),
                })
            else:
                image_id =  image_file.relative_to(generated_image_dir).as_posix()
                print(dict(image_id=image_id, output=text_outputs[0].strip()))
                qa_pairs = parse_qa_pairs(text_outputs[0].strip())
                for (question, ans) in qa_pairs:
                    responses.append({
                    'image_id': image_id,
                        'prompt': prompt, 
                        'raw_answer': f'{question} {ans}',
                        'answer': ans,
                    })

    if dist.is_available() and dist.is_initialized():
        print(f"[RANK {rank + 1}/{world_size}]: Gathering responses from all processors.")
        # prepare a list-of-objects placeholder on each rank
        gathered = [None] * world_size
        dist.all_gather_object(gathered, responses)
        if rank == 0:
            # flatten into single list
            responses = [r for sub in gathered for r in sub]
        else:
            # only rank 0 returns the full list
            return []
    return responses


def geneval_prompt2questions(prompt: str):
    prompt = prompt.lower().strip()
    questions = []

    # Utility: choose "a" or "an"
    def article(word):
        return "an" if word[0] in "aeiou" else "a"

    # 1. Color Attributes: "a photo of a purple tennis racket and a black sink"
    match = re.match(r"a photo of an? (\w+) ([\w\s]+?) and an? (\w+) ([\w\s]+)", prompt)
    if match:
        color1, obj1, color2, obj2 = match.groups()
        if color1.lower() in ['red', 'brown', 'pink', 'yellow', 'orange', 'green', 'blue', 'purple', 'white', 'black',
                              'gray']:
            base_obj1 = obj1.strip().split()[-1]
            base_obj2 = obj2.strip().split()[-1]
            questions.append(f"Is there a {base_obj1}?")
            questions.append(f"Is there a {base_obj2}?")
            questions.append(f"Is the {obj1.strip()} {color1}?")
            questions.append(f"Is the {obj2.strip()} {color2}?")
            return questions

    # 2. Position (supports missing "a/an", multi-word objects)
    match = re.match(
        r"a photo of (?:a[n]? )?([\w\s]+?) (above|below|left of|right of|in front of|behind) (?:a[n]? )?([\w\s]+)",
        prompt)
    if match:
        obj1, relation, obj2 = match.groups()
        questions.append(f"Is there {article(obj1)} {obj1.strip()}?")
        questions.append(f"Is there {article(obj2)} {obj2.strip()}?")
        questions.append(f"Is the {obj1.strip()} {relation} the {obj2.strip()}?")
        return questions

    if " and " not in prompt:
        # 3. Colors: "a photo of a blue rubbish bin"
        match = re.match(r"a photo of an? (\w+) ([\w\s]+)", prompt)
        if match:
            color, obj = match.groups()
            color_words = {'red', 'brown', 'pink', 'yellow', 'orange', 'green', 'blue', 'purple', 'white', 'black',
                           'gray'}
            if color.lower() in color_words:
                base_obj = obj.strip().split()[-1]
                questions.append(f"Is there a {base_obj}?")
                questions.append(f"Is the {obj.strip()} {color}?")
                return questions

        # 4. Counting: "a photo of four dogs"
        match = re.match(r"a photo of (\w+) ([\w\s]+)", prompt)
        if match:
            number, obj = match.groups()
            if number in {"one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten"}:
                plural_obj = obj.strip()
                questions.append(f"Are there {plural_obj}?")
                questions.append(f"Are there {number} {plural_obj}?")
                return questions

        # 5. Single object: "a photo of a cup"
        match = re.match(r"a photo of an? ([\w\s]+)", prompt)
        if match:
            obj = match.group(1)
            questions.append(f"Is there a {obj.strip()}?")
            return questions

    else:
        # 6. Two-object: "a photo of a hair drier and a cake"
        objects = [o.strip() for o in prompt.split("a photo of")[1].split("and")]
        for obj in objects:
            base_obj = obj.strip().split()[-1]
            questions.append(f"Is there a {base_obj}?")
        return questions

    return None

def get_article(word):
    return "an" if word[0] in "aeiou" else "a"

def greedy_samples_byscore(rating_table, topk=4):
    # rating_table
    # image_id, question, answer
    # 00521/samples/00000.png, 'is there an object', 'yes'
    # 00521/samples/00000.png, 'is the object yellow', 'no'
    # 00521/samples/00001.png, 'is there an object', 'yes'
    # 00521/samples/00001.png, 'is the object yellow', 'yes'
    data_table = pd.DataFrame(rating_table)
    data_table['instance_id'] = data_table['image_id'].str.split('/').str[0]

    # Count "yes" answers per image_id and sort descending
    stats = (
        data_table.groupby(['instance_id', 'image_id'])['answer']
        .agg(
            yes_count=lambda x: (x == 'yes').sum(),
            total_questions='count'
        )
        .reset_index()
    )
    stats['score'] = stats['yes_count'] / stats['total_questions']
    print(stats.to_string)
    topk_per_instance = (
        stats
        .sort_values(['instance_id', 'score'], ascending=[True, False])
        .groupby('instance_id', group_keys=False)
        .head(topk)
    )
    return topk_per_instance['image_id'].tolist(), stats


def main():
    config = initialize_config()

    # resolve ckpt path
    ckpt_base_path = config.model.get("local_checkpoints", "")
    if ckpt_base_path:
        config.model.vq_model.vq_model_name = os.path.join(ckpt_base_path, config.model.vq_model.vq_model_name)
        config.model.unigen.llm_model_path = os.path.join(ckpt_base_path, config.model.unigen.llm_model_path)
        config.model.vision_tower.name = os.path.join(ckpt_base_path, config.model.vision_tower.name)

    distributed_state = PartialState()
    device = distributed_state.device
    seed_everything(config.inference.get('seed', 100))
    # load vision encoders
    vq, vision_tower = load_vision_encoders(config, device)

    # load uni-gen model
    unigen, uni_prompting, model_version = load_unigen(config, device)

    with open(config.dataset.validation_prompts_file) as fp:
        geneval_prompts = [json.loads(line) for line in fp]

    generated_images_dir = Path(
        config.generated_images_dir) / f"t2i_samples_step{config.training.generation_timesteps}_scale{config.training.guidance_scale}"
    generated_images_dir.mkdir(exist_ok=True)
    # generate images
    geneval_prompts = geneval_prompts
    generate_images(
        uni_model=unigen,
        uni_prompting=uni_prompting,
        vq_model=vq,
        config=config,
        device=device,
        prompts=geneval_prompts,
        distributed_state=distributed_state,
        output_dir=generated_images_dir,
    )

    # rating images
    conv = conv_templates[model_version].copy()
    # single = 1 question per image, breakdown = break generation prompts into set of VQA
    for mmu_prompt_style in config.inference.mmu_prompt_style.split(','):
        distributed_state.wait_for_everyone()
        rating_table = generate_mmu_rating(
            uni_model=unigen, uni_prompting=uni_prompting, vq_model=vq, vision_tower=vision_tower,
            config=config, device=device,
            mmu_prompt_style=mmu_prompt_style, generated_image_dir=generated_images_dir
        )
        if distributed_state.process_index == 0:
            with open(Path(config.generated_images_dir) / f"rating_table_{mmu_prompt_style}.jsonl", "w") as fp:
                for record in rating_table:
                    fp.write(f"{json.dumps(record)}\n")
            selected_images, stats_table = greedy_samples_byscore(rating_table, topk=4)
            stats_table.to_csv(Path(config.generated_images_dir) / f"stats_table_{mmu_prompt_style}.csv", index=False)
            generated_images_dir_selected = Path(
                config.generated_images_dir) / f"t2i_samples_step{config.training.generation_timesteps}_scale{config.training.guidance_scale}_selected_prompt_{mmu_prompt_style}"
            generated_images_dir_selected.mkdir(exist_ok=True)

            for image_file in selected_images:
                target_dir = generated_images_dir_selected / Path(image_file).parent
                target_dir.mkdir(parents=True, exist_ok=True)
                shutil.copy(generated_images_dir / image_file, generated_images_dir_selected / image_file)
                shutil.copy(
                    generated_images_dir / Path(image_file).parent.parent / "metadata.jsonl",
                    generated_images_dir_selected / Path(image_file).parent.parent / "metadata.jsonl"
                )
            
            if config.inference.get('benchmark', 'geneval') =='dpg':
                img_id_list  = os.listdir(generated_images_dir_selected)
                for img_id in img_id_list:
                    subfolder = os.path.join(str(generated_images_dir_selected), img_id, 'samples')
                    if not os.path.isdir(subfolder):
                        continue
                    image_files = sorted(
                        [f for f in os.listdir(subfolder) if f.endswith('.png')],
                        key=lambda x: int(os.path.splitext(x)[0]))
                    selected_images = image_files[:4]
                    if len(selected_images) < 4:
                        print(f"Warning: {img_id} has less than 4 images, skipping.")
                        continue
                    all_samples = []
                    for img_name in selected_images:
                        img_path = os.path.join(subfolder, img_name)
                        img = Image.open(img_path).convert('RGB')
                        img_tensor = torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0  # HWC -> CHW
                        all_samples.append(img_tensor)
                    
                    all_samples = torch.stack(all_samples, dim=0)  # (4, 3, H, W)
                    grid = make_grid(all_samples, nrow=2)  # 2x2
                    grid_img = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()
                    grid_img = Image.fromarray(grid_img.astype(np.uint8))
                    grid_img.save(generated_images_dir_selected / f"{img_id}.png")
                    shutil.rmtree(generated_images_dir_selected / img_id)
    
    if distributed_state.process_index == 0:
        if config.inference.get('benchmark', 'geneval') =='dpg':
            generated_images_dir_topk = (
                Path(config.generated_images_dir) /
                f"t2i_samples_step{config.training.generation_timesteps}_"
                f"scale{config.training.guidance_scale}_selected_rand4"
            )
            generated_images_dir_topk.mkdir(exist_ok=True)
            img_id_list  = os.listdir(generated_images_dir)
            for img_id in img_id_list:
                subfolder = os.path.join(str(generated_images_dir), img_id, 'samples')
                if not os.path.isdir(subfolder):
                    continue
                image_files = sorted(
                    [f for f in os.listdir(subfolder) if f.endswith('.png')],
                    key=lambda x: int(os.path.splitext(x)[0]))
                selected_images = image_files[:4]
                if len(selected_images) < 4:
                    print(f"Warning: {img_id} has less than 4 images, skipping.")
                    continue
                all_samples = []
                for img_name in selected_images:
                    img_path = os.path.join(subfolder, img_name)
                    img = Image.open(img_path).convert('RGB')
                    img_tensor = torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0  # HWC -> CHW
                    all_samples.append(img_tensor)
                
                all_samples = torch.stack(all_samples, dim=0)  # (4, 3, H, W)
                grid = make_grid(all_samples, nrow=2)  # 2x2
                grid_img = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()
                grid_img = Image.fromarray(grid_img.astype(np.uint8))
                grid_img.save(generated_images_dir_topk / f"{img_id}.png")
    
    # ready for run geneval
    distributed_state.wait_for_everyone()
    print(f"Completed image generation and rating, ready for evaluation!")


if __name__ == "__main__":
    main()
